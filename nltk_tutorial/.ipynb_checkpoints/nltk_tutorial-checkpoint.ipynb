{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download stuff using nltk's download window\n",
    "# stopwords (Corpora), words(Corpora), maxnet_ne_chunker (Models), word2vec_sample (Models),\n",
    "# sample_grammars (Models), porter_test (Models), punkt (Models), averaged_perceptron (Models),\n",
    "# brown (Corpora), treebank (Corpora), words (Corpora)\n",
    "\n",
    "# This opens the download window\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"We are enjoying Pydata session very much\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'enjoying', 'Pydata', 'session', 'very', 'much']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'do', 'this', 'task', ',', 'I', 'am', 'very', 'tired', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = \"I can't do this task, I am very tired.\"\n",
    "word_tokenize(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization might go awry at times.\n",
    "#### can't -> ca + n't -> WTF?!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's a rainy day, I'm enjoying the weather, I like it here.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = \"It's a rainy day, I'm enjoying the weather, I like it here.\"\n",
    "sent_tokenize(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apparently, sentence tokenizers are more reliable than word tokenizers. (To-Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fact: Stopwords contains words from different languages\n",
    "# Fact: Stopwords list might contain repetitive words. Making set can be useful, i.e. obtain unique words.\n",
    "a = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = [\"my\", \"name\", \"is\", \"Mansi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What happens when you read your facts right? -> Making set, i.e. removing repetitions, of stopwords helps our\n",
    "# time complexity when going through the stopwords list, even if it's negligible.\n",
    "# Just saying!!\n",
    "c = [word for word in b if word not in a]\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'rainy',\n",
       " 'day',\n",
       " ',',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'enjoying',\n",
       " 'the',\n",
       " 'weather',\n",
       " ',',\n",
       " 'I',\n",
       " 'like',\n",
       " 'it',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation tokenization\n",
    "a = \"It's a rainy day, I'm enjoying the weather, I like it here.\"\n",
    "wordpunct_tokenize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ^Tokenized punctuations! Mind = blown yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer, WhitespaceTokenizer, SExprTokenizer, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 's',\n",
       " 'a',\n",
       " 'rainy',\n",
       " 'day',\n",
       " 'I',\n",
       " 'm',\n",
       " 'enjoying',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'I',\n",
       " 'like',\n",
       " 'it',\n",
       " 'here']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `RegexpTokenizer`` splits a string into substrings using a regular expression.\n",
    "tknzr = RegexpTokenizer(r'\\w+')\n",
    "tknzr.tokenize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "### The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "### However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Common Stemmers: Porter (Most popular, fast, gentle), Lancaster, Snowball\n",
    "# Common Lematizer: WordNet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = PorterStemmer()\n",
    "l = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter - maximum:  maximum\n",
      "Lancaster - maximum:  maxim\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter - maximum: \", p.stem('maximum'))\n",
    "print(\"Lancaster - maximum: \", l.stem('maximum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter is gentler, Lancaster is slightly more aggresive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('maximum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "### The process of assigning a part-of-speech to each word/token in a sentence. 8 common parts of speech (verb, noun, etc.) but can extend to even 100.\n",
    "### It's non-trivial due to the intricacies of human language. For example, different words can have different meanings in different contexts.\n",
    "### Two broad methods: - Probabilistic approach - Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag, tokenize\n",
    "from nltk.corpus import brown\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\\NNP \n",
      "chased\\VBD \n",
      "the\\DT \n",
      "rabbit\\NN \n",
      ".\\. \n",
      "\n",
      "\n",
      "[('NN', 1), ('DT', 1), ('.', 1), ('NNP', 1), ('VBD', 1)]\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "def get_tags(document):\n",
    "    tokens = tokenize.word_tokenize(document)\n",
    "    for word, pos in pos_tag(tokens):\n",
    "        print(word + \"\\\\\" + pos + \" \")\n",
    "    print(\"\\n\")\n",
    "    tagged_document = pos_tag(tokens)\n",
    "    tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_document)\n",
    "    print(tag_fd.most_common())\n",
    "    print(tag_fd.max())\n",
    "get_tags(\"Alice chased the rabbit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing vs Shallow Parsing\n",
    "### Parsing the sentence would convert the sentence into a tree whose leaves will hold POS tags. A shallow parser or 'chunker' comes somewhere in between a parser and a POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice chased the rabbit.\n",
    "### Breakdown into a parse tree: Alice -> noun phrase, chased the rabbit -> verb phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for third-party softwares\n",
    "### Stanford tagger, NER, Tokenizer and Parser; REPP Tokenizer; CRFSuite for CRF Tagger; Hunpos Tagger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
